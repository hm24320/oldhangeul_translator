import requests
from bs4 import BeautifulSoup
import os
import re
import time
import urllib.parse
from pathlib import Path
from playwright.sync_api import sync_playwright
import shutil

class SejongClassicCrawler:
    def __init__(self, base_url="http://db.sejongkorea.org", download_dir="ì„¸ì¢… í•œê¸€ ê³ ì „"):
        self.base_url = base_url
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)
        self.session = requests.Session()
        
        # ë‹¤ìš´ë¡œë“œ ì¶”ì  íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì½”ë“œì™€ ê°™ì€ ë””ë ‰í† ë¦¬)
        self.downloaded_classics_file = Path(__file__).parent / "downloaded_classic.txt"
        
        # ìš”ì²­ í—¤ë” ì„¤ì • (BeautifulSoupìš©)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Playwright ì´ˆê¸°í™”
        self.playwright = None
        self.browser = None
        self.page = None
        self.temp_download_dir = Path("temp_downloads")
        self.temp_download_dir.mkdir(exist_ok=True)
    
    def get_soup(self, url, retries=3):
        """URLì—ì„œ BeautifulSoup ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not url or not url.startswith('http'):
            print(f"Invalid URL format: {url}")
            return None
        
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return BeautifulSoup(response.text, 'html.parser')
            except requests.exceptions.MissingSchema as e:
                print(f"URL format error for {url}: {e}")
                return None
            except requests.exceptions.RequestException as e:
                print(f"Request error for {url} (attempt {attempt + 1}): {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                else:
                    return None
            except Exception as e:
                print(f"Unexpected error fetching {url} (attempt {attempt + 1}): {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                else:
                    return None
    
    def start_browser(self):
        """Playwright ë¸Œë¼ìš°ì €ë¥¼ ë¹ ë¥´ê²Œ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.playwright is None:
            self.playwright = sync_playwright().start()
            
            # ìµœì í™”ëœ ë¸Œë¼ìš°ì € ì˜µì…˜
            browser_options = {
                'headless': True,
                'args': [
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-images',  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”
                    '--disable-plugins',
                    '--disable-extensions'
                ]
            }
            
            self.browser = self.playwright.chromium.launch(**browser_options)
            
            # ë¹ ë¥¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self.browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                accept_downloads=True,
                ignore_https_errors=True  # SSL ì˜¤ë¥˜ ë¬´ì‹œ
            )
            
            self.page = context.new_page()
            
            # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            self.page.set_default_timeout(30000)
            
            # ë‹¤ì´ì–¼ë¡œê·¸ ìë™ ì²˜ë¦¬ (ê°„ì†Œí™”)
            self.page.on("dialog", lambda dialog: dialog.accept())
    
    def close_browser(self):
        """Playwright ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        try:
            if self.page:
                self.page.close()
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
        except Exception as e:
            print(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ì„ì‹œ ë‹¤ìš´ë¡œë“œ í´ë” ì •ë¦¬
        if self.temp_download_dir.exists():
            shutil.rmtree(self.temp_download_dir, ignore_errors=True)
    
    def sanitize_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        return re.sub(r'[<>:"/\\|?*]', '_', filename).strip()
    
    def load_downloaded_classics(self):
        """ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê³ ì „ ëª©ë¡ì„ íŒŒì¼ì—ì„œ ì½ì–´ì˜µë‹ˆë‹¤."""
        try:
            if self.downloaded_classics_file.exists():
                with open(self.downloaded_classics_file, 'r', encoding='utf-8') as f:
                    downloaded = set()
                    for line in f:
                        title = line.strip()
                        if title:  # ë¹ˆ ì¤„ ì œì™¸
                            downloaded.add(title)
                    return downloaded
            else:
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ì§‘í•© ë°˜í™˜í•˜ê³  íŒŒì¼ ìƒì„±
                self.downloaded_classics_file.touch()
                return set()
        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ê¸°ë¡ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return set()
    
    def save_downloaded_classic(self, title):
        """ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ê³ ì „ì˜ ì œëª©ì„ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ì¡´ ëª©ë¡ í™•ì¸
            downloaded = self.load_downloaded_classics()
            if title not in downloaded:
                with open(self.downloaded_classics_file, 'a', encoding='utf-8') as f:
                    f.write(f"{title}\n")
                print(f"  ğŸ“ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ê¸°ë¡: {title}")
            return True
        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def get_main_links(self):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ê³ ì „ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        main_url = f"{self.base_url}/front/chron.do?type=db&currentPage=1&recordsPerPage=100&cate=table"
        print(f"ë©”ì¸ í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ ì¤‘: {main_url}")
        
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê³ ì „ ëª©ë¡ ë¡œë“œ
        downloaded_classics = self.load_downloaded_classics()
        if downloaded_classics:
            print(f"ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê³ ì „ {len(downloaded_classics)}ê°œë¥¼ ì œì™¸í•©ë‹ˆë‹¤.")
            for classic in list(downloaded_classics)[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"  - {classic}")
            if len(downloaded_classics) > 5:
                print(f"  ... ì™¸ {len(downloaded_classics) - 5}ê°œ")
        
        soup = self.get_soup(main_url)
        if not soup:
            print("ë©”ì¸ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        links = []
        skipped_count = 0
        
        # í…Œì´ë¸”ì—ì„œ ê³ ì „ ë§í¬ë“¤ ì¶”ì¶œ
        # í…Œì´ë¸”ì˜ tbody ì•ˆì—ì„œ ë‘ ë²ˆì§¸ ì—´(ë‚´ìš© ì—´)ì— ìˆëŠ” ë§í¬ë“¤ì„ ì°¾ê¸°
        table = soup.find('table')
        if table:
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:  # ìµœì†Œ 2ê°œ ì—´ì´ ìˆì–´ì•¼ í•¨
                        # ë‘ ë²ˆì§¸ ì—´(ë‚´ìš© ì—´)ì—ì„œ ë§í¬ ì°¾ê¸°
                        content_cell = cells[1]
                        link = content_cell.find('a', href=True)
                        
                        if link:
                            href = link.get('href')
                            title = link.get_text(strip=True)
                            
                            # booklist.do ë˜ëŠ” contentlist.do ë§í¬ë§Œ ì²˜ë¦¬
                            if href and ('booklist.do' in href or 'contentlist.do' in href):
                                # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê³ ì „ì¸ì§€ í™•ì¸
                                if title in downloaded_classics:
                                    print(f"  ê±´ë„ˆëœ€: {title} (ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨)")
                                    skipped_count += 1
                                    continue
                                
                                # URL ë³€í™˜ ê°œì„ 
                                if href.startswith('/'):
                                    href = self.base_url + href
                                elif not href.startswith('http'):
                                    href = self.base_url + '/front/' + href
                                
                                link_type = self.determine_link_type(href)
                                links.append({
                                    'title': title,
                                    'url': href,
                                    'type': link_type
                                })
                                print(f"  ë°œê²¬: {title} ({link_type})")
        
        print(f"ì´ {len(links)}ê°œì˜ ìƒˆë¡œìš´ ê³ ì „ ë§í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ({skipped_count}ê°œ ì œì™¸)")
        return links
    
    def determine_link_type(self, url):
        """ë¹ ë¥´ê²Œ ë§í¬ íƒ€ì…ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        try:
            # ë¹ ë¥¸ ì ‘ì†ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„ ìµœì¢… URL í™•ì¸
            response = self.session.get(url, timeout=15, allow_redirects=True)
            response.raise_for_status()
            
            final_url = response.url
            
            # ìµœì¢… URLë¡œ íƒ€ì… íŒë³„
            if 'contentlist.do' in final_url:
                return 'contentlist'
            elif 'booklist.do' in final_url:
                return 'booklist'
            else:
                # í˜ì´ì§€ ë‚´ìš©ìœ¼ë¡œ ë¹ ë¥¸ íŒë³„
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                if soup.find('ul', class_='dep_01'):
                    return 'contentlist'
                elif soup.find('table', class_='bookListTable'):
                    return 'booklist'
                else:
                    return 'unknown'
                    
        except Exception as e:
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ íŒë³„
            if 'contentlist.do' in url:
                return 'contentlist'
            elif 'booklist.do' in url:
                return 'booklist'
            else:
                return 'unknown'
    
    def get_detail_links_from_contentlist(self, url):
        """contentlist í˜ì´ì§€ì—ì„œ detail ë§í¬ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = self.get_soup(url)
        if not soup:
            return []
        
        detail_links = []
        
        # <ul class="dep_01"> ì•ˆì˜ ê°€ì¥ ìì‹ <ul> íƒœê·¸ë“¤ì„ ì°¾ê¸°
        dep_01_lists = soup.find_all('ul', class_='dep_01')
        
        for dep_01 in dep_01_lists:
            # ê°€ì¥ ê¹Šì€ ë ˆë²¨ì˜ ul íƒœê·¸ë“¤ ì°¾ê¸°
            nested_uls = dep_01.find_all('ul')
            
            for ul in nested_uls:
                # ë” ì´ìƒ ì¤‘ì²©ëœ ulì´ ì—†ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                if not ul.find('ul'):
                    li_elements = ul.find_all('li')
                    for li in li_elements:
                        link = li.find('a', href=True)
                        if link and 'detail.do' in link['href']:
                            detail_url = link['href']
                            # URL ë³€í™˜ ê°œì„ 
                            if detail_url.startswith('/'):
                                detail_url = self.base_url + detail_url
                            elif not detail_url.startswith('http'):
                                detail_url = self.base_url + '/front/' + detail_url
                            
                            detail_links.append({
                                'title': link.get_text(strip=True),
                                'url': detail_url
                            })
        
        return detail_links
    
    def get_detail_links_from_booklist(self, url):
        """booklist í˜ì´ì§€ì—ì„œ contentlist ë§í¬ë“¤ì„ ë¨¼ì € ì°¾ê³ , ê·¸ ë‹¤ìŒ detail ë§í¬ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = self.get_soup(url)
        if not soup:
            return []
        
        detail_links = []
        contentlist_found = False
        
        # <table class="bookListTable"> ì•ˆì˜ <tbody> ì—ì„œ ë§í¬ë“¤ ì°¾ê¸°
        table = soup.find('table', class_='bookListTable')
        if table:
            tbody = table.find('tbody')
            if tbody:
                links = tbody.find_all('a', href=True)
                
                for link in links:
                    href = link['href']
                    # URL ë³€í™˜ ê°œì„ 
                    if href.startswith('/'):
                        href = self.base_url + href
                    elif not href.startswith('http'):
                        href = self.base_url + '/front/' + href
                    
                    # contentlist ë§í¬ì¸ ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ detail ë§í¬ë“¤ ì¶”ì¶œ
                    if 'contentlist.do' in href:
                        contentlist_found = True
                        print(f"  ê¶Œë³„ í˜ì´ì§€ì—ì„œ ìƒì„¸ ë§í¬ ì¶”ì¶œ ì¤‘: {link.get_text(strip=True)}")
                        sub_details = self.get_detail_links_from_contentlist(href)
                        detail_links.extend(sub_details)
                
                # contentlist ë§í¬ê°€ ì—†ëŠ” ê²½ìš°, ì§ì ‘ detail ë§í¬ ì°¾ê¸°
                if not contentlist_found:
                    print("  contentlist ë§í¬ê°€ ì—†ìŒ. ì§ì ‘ detail ë§í¬ ê²€ìƒ‰ ì¤‘...")
                    for link in links:
                        href = link['href']
                        # URL ë³€í™˜ ê°œì„ 
                        if href.startswith('/'):
                            href = self.base_url + href
                        elif not href.startswith('http'):
                            href = self.base_url + '/front/' + href
                        
                        # detail ë§í¬ì¸ ê²½ìš° ì§ì ‘ ì¶”ê°€
                        if 'detail.do' in href:
                            detail_links.append({
                                'title': link.get_text(strip=True),
                                'url': href
                            })
        
        return detail_links
    
    def download_xml_from_detail(self, detail_url, save_dir, filename_prefix=""):
        """detail í˜ì´ì§€ì—ì„œ XML íŒŒì¼ì„ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        function_start = time.time()
        try:
            # ë¸Œë¼ìš°ì €ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì‹œì‘
            if self.page is None:
                self.start_browser()
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # í˜ì´ì§€ ë¡œë”© (ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•)
            page_load_start = time.time()
            self.page.goto(detail_url, wait_until='domcontentloaded', timeout=30000)
            page_load_time = time.time() - page_load_start
            print(f"    â±ï¸ í˜ì´ì§€ ë¡œë”©: {page_load_time:.2f}ì´ˆ")
            
            sleep_start = time.time()
            time.sleep(1)  # ìµœì†Œ ëŒ€ê¸°
            sleep_time = time.time() - sleep_start
            print(f"    â±ï¸ í˜ì´ì§€ ì•ˆì •í™” ëŒ€ê¸°: {sleep_time:.2f}ì´ˆ")
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸° (í•µì‹¬ ì…€ë ‰í„°ë§Œ ì‚¬ìš©)
            button_search_start = time.time()
            download_selectors = [
                'a.btn_down',
                'a[class*="btn_down"]',
                'a[href*="down"]',
                '.btn_down'
            ]
            
            download_element = None
            for selector in download_selectors:
                try:
                    element = self.page.locator(selector).first
                    if element.is_visible():
                        download_element = element
                        break
                except:
                    continue
            button_search_time = time.time() - button_search_start
            print(f"    â±ï¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸°: {button_search_time:.2f}ì´ˆ")
            
            if not download_element:
                function_time = time.time() - function_start
                print(f"    âŒ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì—†ìŒ: {detail_url}")
                print(f"    â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {function_time:.2f}ì´ˆ")
                return False
            
            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
            try:
                download_start = time.time()
                with self.page.expect_download(timeout=30000) as download_info:
                    download_element.click()
                    dialog_wait_start = time.time()
                    time.sleep(1)  # ë‹¤ì´ì–¼ë¡œê·¸ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
                    dialog_wait_time = time.time() - dialog_wait_start
                    print(f"    â±ï¸ ë‹¤ì´ì–¼ë¡œê·¸ ëŒ€ê¸°: {dialog_wait_time:.2f}ì´ˆ")
                
                download = download_info.value
                download_time = time.time() - download_start
                print(f"    â±ï¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {download_time:.2f}ì´ˆ")
                
            except Exception as e:
                # ëŒ€ì•ˆ: ì§ì ‘ ë§í¬ ì ‘ê·¼
                try:
                    alternative_start = time.time()
                    href = download_element.get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            href = self.base_url + href
                        
                        new_page = self.page.context.new_page()
                        with new_page.expect_download(timeout=30000) as download_info:
                            new_page.goto(href)
                        download = download_info.value
                        new_page.close()
                    else:
                        function_time = time.time() - function_start
                        print(f"    â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {function_time:.2f}ì´ˆ")
                        return False
                    alternative_time = time.time() - alternative_start
                    print(f"    â±ï¸ ëŒ€ì•ˆ ë‹¤ìš´ë¡œë“œ ë°©ë²•: {alternative_time:.2f}ì´ˆ")
                except:
                    function_time = time.time() - function_start
                    print(f"    â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {function_time:.2f}ì´ˆ")
                    return False
            
            # íŒŒì¼ëª… ì„¤ì •
            filename_start = time.time()
            try:
                filename = self.sanitize_filename(download.suggested_filename)
            except:
                # ë°±ì—… íŒŒì¼ëª…
                url_parts = urllib.parse.urlparse(detail_url)
                query_params = urllib.parse.parse_qs(url_parts.query)
                record_id = query_params.get('recordId', ['unknown'])[0]
                filename = f"{filename_prefix}_{record_id}.xml" if filename_prefix else f"{record_id}.xml"
                filename = self.sanitize_filename(filename)
            filename_time = time.time() - filename_start
            print(f"    â±ï¸ íŒŒì¼ëª… ì„¤ì •: {filename_time:.2f}ì´ˆ")
            
            # íŒŒì¼ ì €ì¥
            save_start = time.time()
            final_path = save_dir / filename
            download.save_as(final_path)
            save_time = time.time() - save_start
            print(f"    â±ï¸ íŒŒì¼ ì €ì¥: {save_time:.2f}ì´ˆ")
            
            save_wait_start = time.time()
            time.sleep(0.5)  # ì €ì¥ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
            save_wait_time = time.time() - save_wait_start
            print(f"    â±ï¸ ì €ì¥ ì™„ë£Œ ëŒ€ê¸°: {save_wait_time:.2f}ì´ˆ")
            
            # ì €ì¥ ì„±ê³µ í™•ì¸
            if final_path.exists() and final_path.stat().st_size > 0:
                file_size = final_path.stat().st_size
                function_time = time.time() - function_start
                print(f"    âœ… ì €ì¥ ì™„ë£Œ: {filename} ({file_size} bytes)")
                print(f"    â±ï¸ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì‹œê°„: {function_time:.2f}ì´ˆ")
                return True
            else:
                # ëŒ€ì•ˆ: ì§ì ‘ ë³µì‚¬
                try:
                    alternative_save_start = time.time()
                    download_path = Path(download.path())
                    if download_path.exists():
                        shutil.copy2(download_path, final_path)
                        if final_path.exists() and final_path.stat().st_size > 0:
                            alternative_save_time = time.time() - alternative_save_start
                            print(f"    â±ï¸ ëŒ€ì•ˆ ì €ì¥ ë°©ë²•: {alternative_save_time:.2f}ì´ˆ")
                            function_time = time.time() - function_start
                            print(f"    âœ… ì €ì¥ ì™„ë£Œ: {filename}")
                            print(f"    â±ï¸ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì‹œê°„: {function_time:.2f}ì´ˆ")
                            return True
                except:
                    pass
                
                function_time = time.time() - function_start
                print(f"    âŒ ì €ì¥ ì‹¤íŒ¨: {filename}")
                print(f"    â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {function_time:.2f}ì´ˆ")
                return False
            
        except Exception as e:
            function_time = time.time() - function_start
            print(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"    â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {function_time:.2f}ì´ˆ")
            return False
    
    def crawl_classic(self, classic_info):
        """ê°œë³„ ê³ ì „ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        title = classic_info['title']
        url = classic_info['url']
        link_type = classic_info['type']
        
        print(f"\nê³ ì „ í¬ë¡¤ë§ ì‹œì‘: {title}")
        print(f"URL: {url}")
        print(f"íƒ€ì…: {link_type}")
        
        # í´ë” ìƒì„±
        safe_title = self.sanitize_filename(title)
        classic_dir = self.download_dir / safe_title
        classic_dir.mkdir(exist_ok=True)
        
        # ë§í¬ íƒ€ì…ì— ë”°ë¼ detail ë§í¬ë“¤ ì¶”ì¶œ
        detail_links = []
        
        if link_type == 'contentlist':
            detail_links = self.get_detail_links_from_contentlist(url)
        elif link_type == 'booklist':
            detail_links = self.get_detail_links_from_booklist(url)
        
        print(f"  {len(detail_links)}ê°œì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        if len(detail_links) == 0:
            print(f"  ê²½ê³ : '{title}' ê³ ì „ì—ì„œ ìƒì„¸ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"  í˜ì´ì§€ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”: {url}")
        
        # XML íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ
        success_count = 0
        for i, detail in enumerate(detail_links, 1):
            print(f"  [{i}/{len(detail_links)}] {detail['title']} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if not detail.get('url') or not detail['url'].startswith('http'):
                print(f"    ì˜ëª»ëœ URL í˜•ì‹: {detail.get('url', 'None')}")
                continue
            
            if self.download_xml_from_detail(detail['url'], classic_dir, safe_title):
                success_count += 1
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ìµœì†Œ ë”œë ˆì´
            time.sleep(0.5)
        
        print(f"ê³ ì „ '{title}' í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{len(detail_links)} íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
        
        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ê¸°ë¡ (ëª¨ë“  íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ë˜ëŠ” detail ë§í¬ê°€ ì—†ëŠ” ê²½ìš°)
        if len(detail_links) == 0:
            print(f"  âš ï¸ '{title}' ê³ ì „ì— ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            # detail ë§í¬ê°€ ì—†ëŠ” ê²½ìš°ë„ ì™„ë£Œë¡œ ê¸°ë¡ (ì¬ì‹œë„ ë°©ì§€)
            self.save_downloaded_classic(title)
        elif success_count == len(detail_links):
            print(f"  âœ… '{title}' ê³ ì „ì˜ ëª¨ë“  íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
            self.save_downloaded_classic(title)
        elif success_count > 0:
            print(f"  âš ï¸ '{title}' ê³ ì „ì˜ ì¼ë¶€ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({success_count}/{len(detail_links)})")
            print(f"     ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ì‹œë„ë©ë‹ˆë‹¤.")
        else:
            print(f"  âŒ '{title}' ê³ ì „ì˜ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
            print(f"     ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ì‹œë„ë©ë‹ˆë‹¤.")
        
        return success_count
    
    def crawl_all(self):
        """ëª¨ë“  ê³ ì „ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print("ì„¸ì¢…í•œê¸€ê³ ì „ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        try:
            # Playwright ë¸Œë¼ìš°ì € ì‹œì‘
            print("ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.start_browser()
            
            # ë©”ì¸ í˜ì´ì§€ì—ì„œ ëª¨ë“  ê³ ì „ ë§í¬ ì¶”ì¶œ
            classic_links = self.get_main_links()
            
            if not classic_links:
                print("ê³ ì „ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            total_files = 0
            
            # ê° ê³ ì „ë³„ë¡œ í¬ë¡¤ë§ ìˆ˜í–‰
            for i, classic in enumerate(classic_links, 1):
                print(f"\n=== [{i}/{len(classic_links)}] ===")
                
                try:
                    files_downloaded = self.crawl_classic(classic)
                    total_files += files_downloaded
                    
                    # ê° ê³ ì „ ì‚¬ì´ì˜ ìµœì†Œ ë”œë ˆì´
                    time.sleep(1)
                    
                except KeyboardInterrupt:
                    print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                except Exception as e:
                    print(f"ê³ ì „ '{classic['title']}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            
            print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
            print(f"ì´ {len(classic_links)}ê°œ ê³ ì „ì—ì„œ {total_files}ê°œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            print(f"ì €ì¥ ìœ„ì¹˜: {self.download_dir.absolute()}")
            
            # ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ë“¤ í™•ì¸
            print(f"\n=== ì €ì¥ëœ íŒŒì¼ í™•ì¸ ===")
            if self.download_dir.exists():
                all_files = list(self.download_dir.rglob("*.xml"))
                print(f"ì‹¤ì œ ì €ì¥ëœ XML íŒŒì¼ ìˆ˜: {len(all_files)}")
                
                if len(all_files) > 0:
                    print(f"ì €ì¥ëœ íŒŒì¼ë“¤:")
                    for file in all_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                        file_size = file.stat().st_size
                        relative_path = file.relative_to(self.download_dir)
                        print(f"  - {relative_path} ({file_size} bytes)")
                    
                    if len(all_files) > 10:
                        print(f"  ... ì™¸ {len(all_files) - 10}ê°œ íŒŒì¼")
                        
                    # í´ë”ë³„ íŒŒì¼ ìˆ˜ í†µê³„
                    folder_stats = {}
                    for file in all_files:
                        folder = file.parent.name
                        folder_stats[folder] = folder_stats.get(folder, 0) + 1
                    
                    print(f"\ní´ë”ë³„ íŒŒì¼ ìˆ˜:")
                    for folder, count in folder_stats.items():
                        print(f"  - {folder}: {count}ê°œ íŒŒì¼")
                else:
                    print("âŒ ì €ì¥ëœ XML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                    print("ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì œì•ˆ:")
                    print("1. ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸")
                    print("2. ì‚¬ì´íŠ¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸")
                    print("3. Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í™•ì¸: playwright install chromium")
            else:
                print(f"âŒ ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.download_dir.absolute()}")
            
        except KeyboardInterrupt:
            print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            # ë¸Œë¼ìš°ì € ì •ë¦¬
            print("ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
            self.close_browser()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ì„¸ì¢…í•œê¸€ê³ ì „ í¬ë¡¤ëŸ¬ ===")
    print("ì£¼ì˜: ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Playwright ë¸Œë¼ìš°ì €ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    print("  pip install playwright")
    print("  playwright install chromium")
    print("=" * 50)
    
    try:
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        crawler = SejongClassicCrawler()
        
        # í¬ë¡¤ë§ ì‹œì‘
        crawler.crawl_all()
        
    except ImportError as e:
        print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        print("\ní•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("  pip install -r requirements.txt")
        print("  playwright install chromium")
    except Exception as e:
        print(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nPlaywrightê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("  pip install playwright")
        print("  playwright install chromium")

if __name__ == "__main__":
    main() 